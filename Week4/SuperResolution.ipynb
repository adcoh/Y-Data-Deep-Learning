{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #download dataset\n",
    "# !wget -N http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "# !tar -xzf VOCtrainval_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import models \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "# wandb.login()\n",
    "wandb.init(project=\"Y-Data-DL-Week4-Super-Resolution-Final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2007Dataset(Dataset):\n",
    "    \"\"\" \n",
    "    \n",
    "    create Dataset class that takes input of PascalVOC dataset and creates sets of images of \n",
    "    sizes X - 72x72x3, y_mid – 144x144x3, y_large – 288x288x3 \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_set='trainval', root=None, transform=None, sample_slice=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_set (str): one of 'train', 'trainval', or 'val', default \"trainval\"\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            sample_slice (list or tuple): a 2-value list or tuple indicating subset of files to be selected from the dataset\n",
    "        \"\"\"\n",
    "        super(VOC2007Dataset).__init__()\n",
    "        self.transform = transform\n",
    "        \n",
    "        if root is None:\n",
    "            root = os.path.abspath(os.path.curdir)\n",
    "        self.root = root\n",
    "        valid_sets = ['train', 'trainval', 'val']\n",
    "        assert (image_set in valid_sets), f\"{image_set} not among the allowed values. Allowed values are {', '.join(valid_sets)}\"\n",
    "        \n",
    "        base_dir = os.path.join('VOCdevkit', 'VOC2007')\n",
    "        voc_root = os.path.join(self.root, base_dir)\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "        \n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "        \n",
    "        if sample_slice is None:\n",
    "            sample_slice = (0, len(file_names))\n",
    "        assert (sample_slice[-1] <= len(file_names)), f'sample_slice indices are out bounds. Maximum indices: {len(file_names)-1}'\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names[sample_slice[0]:sample_slice[-1]]]\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
    "        \"\"\"\n",
    "        img = cv2.imread(self.images[index], cv2.IMREAD_COLOR)\n",
    "        img_large = cv2.resize(img, (288, 288))\n",
    "        img_mid = cv2.resize(img, (144, 144))\n",
    "        img_small = cv2.resize(img, (72, 72))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img_large = self.transform(img_large)\n",
    "            img_mid = self.transform(img_mid)\n",
    "            img_small = self.transform(img_small)\n",
    "\n",
    "        return dict(y_large=img_large, y_mid=img_mid, X=img_small)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update(dict(batch_size=5, epochs=10, lr=0.01, no_cuda=True,\n",
    "                                 seed=42, log_interval=10))        # Initialize config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config\n",
    "use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "random.seed(config.seed)       # python random seed\n",
    "torch.manual_seed(config.seed) # pytorch random seed\n",
    "np.random.seed(config.seed) # numpy random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = VOC2007Dataset(image_set='trainval', transform=transform, sample_slice=[0,1000]) # training dataset\n",
    "trainloader = DataLoader(trainset, batch_size=config.batch_size, shuffle=False, **kwargs)\n",
    "testset = VOC2007Dataset(image_set='trainval', transform=transform, sample_slice=[-101, -1]) # validation dataset\n",
    "testloader = DataLoader(testset, batch_size=config.batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(image_dict):\n",
    "    \"\"\"Function to display images from three-image-tuple from VOC2007Dataset class\n",
    "    Args:\n",
    "        image_dict (dict): contains three different-sized image tensors of the same content \n",
    "    \"\"\"\n",
    "    f = plt.figure(figsize=(16, 6))\n",
    "    n = 1\n",
    "    for key, img in image_dict.items():\n",
    "        ax = f.add_subplot(1, len(image_dict), n)\n",
    "        img = img.detach().permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.title.set_text(key)\n",
    "        n += 1\n",
    "#     return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(trainset[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(convnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.upsample1 = nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1)\n",
    "#         self.upsample1 = nn.Upsample(scale_factor=2.0, mode='nearest')\n",
    "        self.conv3 = nn.Conv2d(64, 3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print('input shape: ', x.shape)\n",
    "        x = self.conv1(x)\n",
    "#         print('1st layer output shape: ', x.shape)\n",
    "        x = self.conv2(F.leaky_relu(x))\n",
    "#         print('2nd layer output shape: ', x.shape)\n",
    "        x = self.upsample1(F.leaky_relu(x), output_size=(144, 144))\n",
    "#         print('Upsample layer output shape: ', x.shape)\n",
    "        x = self.conv3(F.leaky_relu(x))\n",
    "#         print('final layer output shape: ', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_y_mid(config, net, train_data, optimizer, epoch, loss_fn=None):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.MSELoss()\n",
    "    for i, batch in tqdm(enumerate(train_data), total=len(train_data)):\n",
    "        X = batch['X']\n",
    "        y_mid = batch['y_mid']\n",
    "        optimizer.zero_grad()\n",
    "        output = net(X)\n",
    "        loss = loss_fn(output, y_mid)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    wandb.log({'Train Loss - Model 1': train_loss/len(train_data), 'Epoch': epoch}, commit=False)\n",
    "\n",
    "def test_model_y_mid(config, net, test_data, epoch, loss_fn=None):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    example_images = []\n",
    "    avg_psnr = 0\n",
    "    n = len(test_data)\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for j, batch in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "            X = batch['X']\n",
    "            y_mid = batch['y_mid']\n",
    "            output = net(X)\n",
    "            loss = loss_fn(output, y_mid).item()\n",
    "            test_loss += loss\n",
    "            avg_psnr += 10 * np.log10(1/loss)\n",
    "            example_images.append(wandb.Image(transforms.ToPILImage(mode='RGB')(output[0]), \n",
    "                                              caption=\"Output Reconstruction\"))\n",
    "            example_images.append(wandb.Image(transforms.ToPILImage(mode='RGB')(y_mid[0]), \n",
    "                                              caption=\"Target\"))\n",
    "    wandb.log({'Test Loss - Model 1': test_loss/n, \"Examples\": example_images, 'Epoch': epoch, \n",
    "              'Avg Peak Signal To Noise Ratio': avg_psnr/n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(network):\n",
    "    model = network().to(device)\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        train_model_y_mid(config, model, trainloader, optimizer, epoch, loss_fn)\n",
    "        test_model_y_mid(config, model, testloader, epoch, loss_fn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = run_training(convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "test = next(dataiter)\n",
    "y_mid = model1(test['X'])\n",
    "img_dct = dict(y_mid=y_mid[0], X=test['X'][0])\n",
    "show_images(img_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convnet2(convnet):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(convnet2, self).__init__()\n",
    "        self.upsample2 = nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(64, 3, 1)\n",
    "        self.conv3_2 = nn.Conv2d(64, 3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        x = self.conv1(x)\n",
    "#         print('1st layer output shape: ', x.shape)\n",
    "        x = self.conv2(F.leaky_relu(x))\n",
    "#         print('2nd layer output shape: ', x.shape)\n",
    "        x = self.upsample1(x, output_size=(h*2, w*2))\n",
    "        h, w = x.shape[-2:]\n",
    "#         print('Upsample layer output shape: ', x.shape)\n",
    "        x = F.leaky_relu(x)\n",
    "        x_mid = self.conv3_1(x)\n",
    "        # print('final layer x_mid output shape: ', x_mid.shape)\n",
    "        x_large = self.upsample2(x, output_size=(h*2, w*2))\n",
    "        x_large = self.conv3_2(F.leaky_relu(x_large))\n",
    "        # print('final layer x_large output shape: ', x_large.shape)\n",
    "        return x_mid, x_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_y_mid_large(config, net, train_data, optimizer, epoch, loss_fn=None, model_number=2):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.MSELoss()\n",
    "    for i, batch in tqdm(enumerate(train_data), total=len(train_data)):\n",
    "        X = batch['X']\n",
    "        y_mid = batch['y_mid']\n",
    "        y_large = batch['y_large']\n",
    "        optimizer.zero_grad()\n",
    "        mid_output, large_output = net(X)\n",
    "        loss = loss_fn(mid_output, y_mid) + loss_fn(large_output, y_large)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    wandb.log({f'Train Loss - Model {model_number}': train_loss/len(train_data), 'Epoch': epoch}, commit=False)\n",
    "\n",
    "def test_model_y_mid_large(config, net, test_data, epoch, loss_fn=None, model_number=2):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    example_images = []\n",
    "    avg_psnr = 0\n",
    "    n = len(test_data)\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for j, batch in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "            X = batch['X'].to(device)\n",
    "            y_mid = batch['y_mid'].to(device)\n",
    "            y_large = batch['y_large'].to(device)\n",
    "            mid_output, large_output = net(X)\n",
    "            loss = loss_fn(mid_output, y_mid).item() + loss_fn(large_output, y_large).item() \n",
    "            test_loss += loss\n",
    "            avg_psnr += 10 * np.log10(1/loss)\n",
    "            example_images.append(wandb.Image(transforms.ToPILImage(mode='RGB')(mid_output[0]), \n",
    "                                                caption=\"Mid Output Reconstruction\"))\n",
    "            example_images.append(wandb.Image(transforms.ToPILImage(mode='RGB')(large_output[0]), \n",
    "                                                caption=\"Large Output Reconstruction\"))\n",
    "            example_images.append(wandb.Image(transforms.ToPILImage(mode='RGB')(y_mid[0]), \n",
    "                                                caption=\"Target\"))\n",
    "    wandb.log({f'Test Loss - Model {model_number}': test_loss/len(test_data), \"Examples\": example_images, 'Epoch': epoch,\n",
    "                'Avg Peak Signal To Noise Ratio': avg_psnr/n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training2(network, model_number):\n",
    "    model = network().to(device)\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    wandb.re\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        train_model_y_mid_large(config, model, trainloader, optimizer, epoch, loss_fn=loss_fn, model_number=model_number)\n",
    "        test_model_y_mid_large(config, model, testloader, epoch, loss_fn=loss_fn, model_number=model_number)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model2 = run_training2(convnet2, model_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "test = next(dataiter)\n",
    "y_mid, y_large = model2(test['X'])\n",
    "img_dct = dict(y_large=y_large[0], y_mid=y_mid[0], X=test['X'][0])\n",
    "show_images(img_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(resnet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 1)\n",
    "        self.res_block1_1 = nn.Conv2d(32,32, 3, padding=1)\n",
    "        self.res_block1_2 = nn.Conv2d(32,32, 3, padding=1)\n",
    "        self.res_block2_1 = nn.Conv2d(32,32, 3, padding=1)\n",
    "        self.res_block2_2 = nn.Conv2d(32,32, 3, padding=1)\n",
    "        self.upsample1 = nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.res_block3_1 = nn.Conv2d(32,32, 3, padding=1)\n",
    "        self.res_block3_2 = nn.Conv2d(32,32, 3, padding=1)\n",
    "        self.upsample2 = nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(32, 3, 1)\n",
    "        self.conv3_2 = nn.Conv2d(32, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print('input shape: ', x.shape)\n",
    "        h, w = x.shape[-2:]\n",
    "        x = self.conv1(x)\n",
    "        # print('1st layer output shape: ', x.shape)\n",
    "        x_res = self.res_block1_1(x)\n",
    "        x_res = self.res_block1_2(x_res)\n",
    "        x = F.leaky_relu(x + x_res)\n",
    "        # print('1st residual layer output shape: ', x.shape)\n",
    "        x_res = self.res_block2_1(x)\n",
    "        x_res = self.res_block2_2(x_res)\n",
    "        x = F.leaky_relu(x_res + x)\n",
    "        # print('2nd residual layer output shape: ', x.shape)\n",
    "        x = self.upsample1(x, output_size=(h*2, w*2))\n",
    "        h, w = x.shape[-2:]\n",
    "        # print('1st Upsample layer output shape: ', x.shape)\n",
    "        x_mid = self.conv3_1(F.leaky_relu(x))\n",
    "        # print('final layer x_mid output shape: ', x_mid.shape)\n",
    "        x_res = self.res_block3_1(x)\n",
    "        x_res = self.res_block3_2(x_res)\n",
    "        x = F.leaky_relu(x_res + x)\n",
    "        x_large = self.upsample2(x, output_size=(h*2, w*2))\n",
    "        # print('final layer x_large output shape: ', x_large.shape)\n",
    "        x_large = self.conv3_2(F.leaky_relu(x_large))\n",
    "        return x_mid, x_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = run_training2(resnet, model_number=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "test = next(dataiter)\n",
    "y_mid, y_large = model3(test['X'])\n",
    "img_dct = dict(y_large=y_large[0], y_mid=y_mid[0], X=test['X'][0])\n",
    "show_images(img_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dilation_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(dilation_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 1)\n",
    "        self.dil_block1_1 = nn.Conv2d(32,32, 3, padding=1, dilation=1)\n",
    "        self.dil_block1_2 = nn.Conv2d(32,32, 3, padding=2, dilation=2)\n",
    "        self.dil_block1_3 = nn.Conv2d(32,32, 3, padding=4, dilation=4)\n",
    "        self.dil_block1_4 = nn.Conv2d(96,32, 3, padding=1)\n",
    "        self.dil_block2_1 = nn.Conv2d(32,32, 3, padding=1, dilation=1)\n",
    "        self.dil_block2_2 = nn.Conv2d(32,32, 3, padding=2, dilation=2)\n",
    "        self.dil_block2_3 = nn.Conv2d(32,32, 3, padding=4, dilation=4)\n",
    "        self.dil_block2_4 = nn.Conv2d(96,32, 3, padding=1)\n",
    "        self.dil_block3_1 = nn.Conv2d(32,32, 3, padding=1, dilation=1)\n",
    "        self.dil_block3_2 = nn.Conv2d(32,32, 3, padding=2, dilation=2)\n",
    "        self.dil_block3_3 = nn.Conv2d(32,32, 3, padding=4, dilation=4,)\n",
    "        self.dil_block3_4 = nn.Conv2d(96,32, 3, padding=1)\n",
    "        self.upsample1 = nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.upsample2 = nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(32, 3, 1)\n",
    "        self.conv3_2 = nn.Conv2d(32, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        x = self.conv1(x)\n",
    "        # print('1st layer output shape: ', x.shape)\n",
    "        x1 = self.dil_block1_1(x)\n",
    "        # print('1st dilation output shape: ', x1.shape)\n",
    "        x2 = self.dil_block1_2(x)\n",
    "        # print('2nd dilation output shape: ', x2.shape)\n",
    "        x3 = self.dil_block1_3(x)\n",
    "        # print('3rd dilation output shape: ', x3.shape)\n",
    "        x = self.dil_block1_4(F.leaky_relu(torch.cat([x1, x2, x3], dim=1)))\n",
    "        # print('1st dilation layer output shape: ', x.shape)\n",
    "        x1 = self.dil_block2_1(x)\n",
    "        x2 = self.dil_block2_2(x)\n",
    "        x3 = self.dil_block2_3(x)\n",
    "        x = self.dil_block2_4(F.leaky_relu(torch.cat([x1, x2, x3], dim=1)))\n",
    "        # print('2nd dilation layer output shape: ', x.shape)\n",
    "        x = self.upsample1(x, output_size=(h*2, w*2))\n",
    "        h, w = x.shape[-2:]\n",
    "        # print('1st Upsample layer output shape: ', x.shape)\n",
    "        x_mid = self.conv3_1(F.leaky_relu(x))\n",
    "        # print('final layer x_mid output shape: ', x_mid.shape)\n",
    "        x1 = self.dil_block3_1(x)\n",
    "        x2 = self.dil_block3_2(x)\n",
    "        x3 = self.dil_block3_3(x)\n",
    "        x = self.dil_block3_4(F.leaky_relu(torch.cat([x1, x2, x3], dim=1)))\n",
    "        x_large = self.upsample2(x, output_size=(h*2, w*2))\n",
    "        # print('final layer x_large output shape: ', x_large.shape)\n",
    "        x_large = self.conv3_2(F.leaky_relu(x_large))\n",
    "        return x_mid, x_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = run_training2(dilation_net, model_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "test = next(dataiter)\n",
    "y_mid, y_large = model4(test['X'])\n",
    "img_dct = dict(y_large=y_large[0], y_mid=y_mid[0], X=test['X'][0])\n",
    "show_images(img_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pretrained_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(pretrained_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.upsample1 = nn.ConvTranspose2d(128, 128, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.upsample2 = nn.ConvTranspose2d(128, 128, 3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 3, 1)\n",
    "        self.backbone = models.vgg16(pretrained=True)._modules['features'][:4]\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.mean = torch.Tensor([0.485,0.456, 406]).reshape(1,3,1,1)\n",
    "        self.std = torch.Tensor([0.229, 0.224, 0.225]).reshape(1,3,1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        out = self.backbone((x-self.mean)/self.std)\n",
    "        # print('backbone layer output shape: ', out.shape)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        # print('1st layer output shape: ', x.shape)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        # print('2nd layer output shape: ', x.shape)\n",
    "        x = torch.cat([x, out], dim=1)\n",
    "        # print('Concatenated output shape: ', x.shape)\n",
    "        x = self.upsample1(x, output_size=(h*2, w*2))\n",
    "        # print('1st upsample layer output shape: ', x.shape)\n",
    "        h, w = x.shape[-2:]\n",
    "        x_mid = F.leaky_relu(self.conv3(x))\n",
    "        # print('Mid_output shape: ', x_mid.shape)\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        # print('4th conv layer output shape: ', x.shape)\n",
    "        x_large = self.upsample2(x, output_size=(h*2, w*2))\n",
    "        # print('2nd upsample layer output shape: ', x_large.shape)\n",
    "        x_large = F.leaky_relu(self.conv5(x_large))\n",
    "        # print('x_large output shape: ', x_large.shape)\n",
    "        return x_mid, x_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = run_training2(pretrained_net, model_number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "test = next(dataiter)\n",
    "y_mid, y_large = model5(test['X'])\n",
    "img_dct = dict(y_large=y_large[0], y_mid=y_mid[0], X=test['X'][0])\n",
    "show_images(img_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pixel_shuffle_net(pretrained_net):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(pixel_shuffle_net, self).__init__()\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 192, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(32, 3, 1)\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        out = self.backbone(self.normalizer(x))\n",
    "        # print('backbone layer output shape: ', out.shape)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        # print('1st layer output shape: ', x.shape)\n",
    "        x = F.leaky_relu(self.conv2_1(x))\n",
    "        # print('2_1 layer output shape: ', x.shape)\n",
    "        x = F.leaky_relu(self.conv2_2(x))\n",
    "        # print('2_2 layer output shape: ', x.shape)\n",
    "        x = torch.cat([x, out], dim=1)\n",
    "        # print('Concatenated output shape: ', x.shape)\n",
    "        x = F.pixel_shuffle(x, upscale_factor=2)\n",
    "        # print('1st upsample layer output shape: ', x.shape)\n",
    "        h, w = x.shape[-2:]\n",
    "        x_mid = F.leaky_relu(self.conv3(x))\n",
    "        # print('Mid_output shape: ', x_mid.shape)\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        # print('4th conv layer output shape: ', x.shape)\n",
    "        x_large = F.pixel_shuffle(x, upscale_factor=2)\n",
    "        # print('2nd upsample layer output shape: ', x_large.shape)\n",
    "        x_large = F.leaky_relu(self.conv5(x_large))\n",
    "        # print('x_large output shape: ', x_large.shape)\n",
    "        return x_mid, x_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = run_training2(pixel_shuffle_net, model_number=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "test = next(dataiter)\n",
    "y_mid, y_large = model6(test['X'])\n",
    "img_dct = dict(y_large=y_large[0], y_mid=y_mid[0], X=test['X'][0])\n",
    "show_images(img_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('yandex': virtualenv)",
   "language": "python",
   "name": "python37564bityandexvirtualenv9d32818e7464483c97cb7ec262d53e40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}